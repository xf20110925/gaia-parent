a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.zookeeperConnect = 192.168.5.35:2181,192.168.5.36:2181,192.168.5.37:2181
a1.sources.r1.topic = media_basic_info
a1.sources.r1.groupId = gaia
a1.sources.r1.kafka.consumer.timeout.ms = 1000

a1.sources.r1.interceptors = ptb-message
a1.sources.r1.interceptors.ptb-message.type = com.ptb.gaia.etl.flume.interceptor.GaiaMessageInterceptor$Builder
  
# Use a channel which buffers events in memory
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /data/gaia/flume/a1/checkpoint
a1.channels.c1.dataDirs = /data/gaia/flume/a1/data
a1.channels.c1.transactionCapacity = 2000
a1.channels.c1.checkpointInterval = 3000
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 200000
  
#a1.channels.c1.type = memory
#a1.channels.c1.capacity = 10000000
#a1.channels.c1.transactionCapacity = 10000
  
#a1.channels.c1.byteCapacityBufferPercentage = 40
#a1.channels.c1.byteCapacity = 80000000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

a1.sinks.k1.type = com.ptb.gaia.etl.flume.sink.MongoSink
a1.sinks.k1.gaia.flume.sink.process.class = com.ptb.gaia.etl.flume.process.MediaStaticProcess
a1.sinks.k1.gaia.flume.sink.batch.size=1500


#---------------------------------agent a1:article dynamic info--------------------------
a2.sources = r1
a2.sinks = k1
a2.channels = c1

a2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r1.zookeeperConnect=192.168.5.35:2181,192.168.5.36:2181,192.168.5.37:2181
a2.sources.r1.topic = article_spread_info
a2.sources.r1.groupId = gaia
a2.sources.r1.kafka.consumer.timeout.ms = 1000

a2.sources.r1.interceptors = ptb-message
a2.sources.r1.interceptors.ptb-message.type = com.ptb.gaia.etl.flume.interceptor.GaiaMessageInterceptor$Builder


# Use a channel which buffers events in memory
a2.channels.c1.type = file
a2.channels.c1.checkpointDir = /data/gaia/flume/a2/checkpoint
a2.channels.c1.dataDirs = /data/gaia/flume/a2/data
a2.channels.c1.transactionCapacity = 2000
a2.channels.c1.checkpointInterval = 3000
a2.channels.c1.maxFileSize = 2146435071
a2.channels.c1.capacity = 200000

# # Bind the source and sink to the channel
a2.sources.r1.channels = c1
a2.sinks.k1.channel = c1
#
a2.sinks.k1.type = com.ptb.gaia.etl.flume.sink.MongoSink
a2.sinks.k1.gaia.flume.sink.process.class = com.ptb.gaia.etl.flume.process.ArticleDynamicProcess
a2.sinks.k1.gaia.flume.sink.batch.size=1500

#---------------------------------agent a1:media dynamic info--------------------------
a3.sources = r1
a3.sinks = k1
a3.channels = c1

a3.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a3.sources.r1.zookeeperConnect = 192.168.5.35:2181,192.168.5.36:2181,192.168.5.37:2181
a3.sources.r1.topic = media_spread_info
a3.sources.r1.groupId = gaia
a3.sources.r1.kafka.consumer.timeout.ms = 1000

a3.sources.r1.interceptors = ptb-message
a3.sources.r1.interceptors.ptb-message.type = com.ptb.gaia.etl.flume.interceptor.GaiaMessageInterceptor$Builder


# Use a channel which buffers events in memory
a3.channels.c1.type = file
a3.channels.c1.checkpointDir = /data/gaia/flume/a3/checkpoint
a3.channels.c1.dataDirs = /data/gaia/flume/a3/data
a3.channels.c1.transactionCapacity = 2000
a3.channels.c1.checkpointInterval = 3000
a3.channels.c1.maxFileSize = 2146435071
a3.channels.c1.capacity = 200000

# # Bind the source and sink to the channel
a3.sources.r1.channels = c1
a3.sinks.k1.channel = c1
#
a3.sinks.k1.type = com.ptb.gaia.etl.flume.sink.MongoSink
a3.sinks.k1.gaia.flume.sink.process.class = com.ptb.gaia.etl.flume.process.MediaDynamicProcess
a3.sinks.k1.gaia.flume.sink.batch.size=1500

#---------------------------------agent a1:article static info--------------------------
a4.sources = r1
a4.sinks = k1
a4.channels = c1

a4.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a4.sources.r1.zookeeperConnect = 192.168.5.35:2181,192.168.5.36:2181,192.168.5.37:2181
a4.sources.r1.topic = article_basic_info
a4.sources.r1.groupId = gaia
a4.sources.r1.kafka.consumer.timeout.ms = 1000

a4.sources.r1.interceptors = ptb-message
a4.sources.r1.interceptors.ptb-message.type = com.ptb.gaia.etl.flume.interceptor.GaiaMessageInterceptor$Builder


# Use a channel which buffers events in memory
a4.channels.c1.type = file
a4.channels.c1.checkpointDir = /data/gaia/flume/a4/checkpoint
a4.channels.c1.dataDirs = /data/gaia/flume/a4/data
a4.channels.c1.transactionCapacity = 2000
a4.channels.c1.checkpointInterval = 3000
a4.channels.c1.maxFileSize = 2146435071
a4.channels.c1.capacity = 200000

# # Bind the source and sink to the channel
a4.sources.r1.channels = c1
a4.sinks.k1.channel = c1
#
a4.sinks.k1.type = com.ptb.gaia.etl.flume.sink.MongoSink
a4.sinks.k1.gaia.flume.sink.process.class = com.ptb.gaia.etl.flume.process.ArticleStaticProcess
a4.sinks.k1.gaia.flume.sink.batch.size=1500
